{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as stats2\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import random\n",
    "import inspect\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prueba = pd.DataFrame({\n",
    "\"ES_NO_ES\":[np.random.choice(['s','n']) for _ in range(1000)],\n",
    "\"sexo\":[np.random.choice(['h','m']) for _ in range(1000)],\n",
    "\"Datos_C\":[np.random.choice([0,1]) for _ in range(1000)],\n",
    "\"Datos_D\": list(np.random.standard_normal(1000)),\n",
    "\"Datos_E\": list(np.random.standard_normal(1000)),\n",
    "\"Datos_Poisson_1\": list( stats.poisson.rvs(mu=4, size=1000)),\n",
    "\"Datos_Poisson_3\": list( np.random.poisson(lam=10, size=1000)),\n",
    "\"Datos_Geom\": list( stats.geom.rvs(0.75, size=1000)),\n",
    "\"Datos_F\": [np.random.randint(0,1000) for _ in range(1000)],\n",
    "\"Datos_G\": [np.random.randint(0,1000) for _ in range(1000)],\n",
    "\"Datos_cate_A\": ['Grupo '+str(np.random.randint(0,6)) for _ in range(1000)],\n",
    "\"Datos_cate_B\": ['Grupo '+str(np.random.randint(0,4)) for _ in range(1000)],\n",
    "\"Datos_cate_C\": [np.random.randint(0,60) for _ in range(1000)],\n",
    "\n",
    "})\n",
    "\n",
    "# for i in range(1,6):\n",
    "#     df_prueba['Datos_E'][random.randint(0,23)]=None\n",
    "\n",
    "\n",
    "# for i in range(1,10):\n",
    "#     df_prueba['Datos_F'][random.randint(0,23)]=None\n",
    "\n",
    "# for i in range(0,11):\n",
    "#     df_prueba['Datos_G'][i]=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DF_exploracion(pd.DataFrame):\n",
    "\n",
    "    def __init__(self, *args, **kw):\n",
    "        super(DF_exploracion, self).__init__(*args, **kw)\n",
    "        self.cuanti=pd.DataFrame\n",
    "        self.cuanti_antes_de_outliers_y_inputs=pd.DataFrame\n",
    "        self.cuali=pd.DataFrame\n",
    "        self.dico=pd.DataFrame\n",
    "        self.cate=pd.DataFrame\n",
    "        self.eliminado=pd.DataFrame\n",
    "        self.dummy=pd.DataFrame\n",
    "        self.df=pd.DataFrame\n",
    "        self.df_inputado=pd.DataFrame\n",
    "        self.df_limpio=pd.DataFrame\n",
    "        self.normal_cuatis=[]\n",
    "        self.normal_grupos_dico=[]\n",
    "        self.normal_grupos_cate=[]\n",
    "        self.discreta=[]\n",
    "        self.stingg=[]\n",
    "        \n",
    "        \n",
    "        self.porcentaje_nulos_permitido=0.6\n",
    "\n",
    "    def variables(self):\n",
    "\n",
    "        dico=[]\n",
    "        cuantis=[]\n",
    "        categori=[]\n",
    "        eliminar=[]\n",
    "        \n",
    "\n",
    "        for i in self.columns: \n",
    "\n",
    "            try:\n",
    "                datos=self[i].dropna().to_numpy()\n",
    "                discreta=True\n",
    "                for j in datos:\n",
    "                    if (j%1 !=0):\n",
    "                        discreta=False\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                if (discreta):\n",
    "                    self.discreta.append(i)\n",
    "            except:\n",
    "                self.stingg.append(i)\n",
    "\n",
    "            nulos= (self[i].isnull().sum())/len(self[i])\n",
    "            \n",
    "            if ((len(self[i].dropna().unique())==2) and (nulos<=self.porcentaje_nulos_permitido)):\n",
    "                tipo_de_var=f\"{len(self[i].dropna().unique())} tipos, posiblemente: DICOTOMICA\"\n",
    "                dico.append(i)\n",
    "\n",
    "            elif ((len(self[i].dropna().unique())>10) and  (nulos<=self.porcentaje_nulos_permitido)):\n",
    "                tipo_de_var=f\"{len(self[i].dropna().unique())} tipos, posiblemente: CUANTITATIVA\"\n",
    "                cuantis.append(i)\n",
    "\n",
    "            elif ( (len(self[i].dropna().unique())<2) or (nulos>self.porcentaje_nulos_permitido)):\n",
    "                tipo_de_var=f\"SOLO {len(self[i].dropna().unique())} TIPOS, NO VALE LA COLUMNA\"\n",
    "                eliminar.append(i)\n",
    "            else:\n",
    "                tipo_de_var=f\"{len(self[i].dropna().unique())} tipos, posiblemente: CATEGORICA/CUANTI\"\n",
    "                categori.append(i)\n",
    "\n",
    "            print (f\"|  {i} \\n|   - Tipo de dato: {self[i].dtype} \\n|   - Valores repetidos: {tipo_de_var} \\n|   - Nulos: {nulos} \\n| \")\n",
    "\n",
    "        print (f\"|----------------------------------------------------------------------------------------------------\\n|  TODAS: {self.columns} \\n|  DICOTOMICAS: {dico} \\n|  CATEGORICAS: {categori} \\n|  CUANTITATIVAS: {cuantis} \\n|  ELIMINAR: {eliminar}\")\n",
    "        print(\"|----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        self.DF_cuantis(cuantis)\n",
    "        self.DF_cualis(categori+dico)\n",
    "        self.DF_dicotomica(dico)\n",
    "        self.DF_categorica(categori)\n",
    "        self.DF_elimiminado(eliminar)\n",
    "        self.df=self\n",
    "        \n",
    "    def todas_col(self):\n",
    "        return self.df\n",
    "    \n",
    "    def DF_cuantis(self,lista):\n",
    "        self.cuanti=self[lista]\n",
    "\n",
    "    def DF_elimiminado(self,lista):\n",
    "        self.eliminado=self[lista]\n",
    "        \n",
    "    def DF_cualis(self,lista):\n",
    "        self.cuali=self[lista]\n",
    "        \n",
    "    def DF_dicotomica(self, lista):\n",
    "        self.dico=self[lista]\n",
    "        \n",
    "    def DF_categorica(self, lista):\n",
    "        self.cate=self[lista]   \n",
    "\n",
    "\n",
    "\n",
    "    def limpiar_aux(self):\n",
    "        \n",
    "        try:\n",
    "            df_nuevo=pd.DataFrame\n",
    "            aux1=list(self.dico.columns)\n",
    "            aux=[]\n",
    "            df_nuevo=pd.get_dummies(self.df, columns=aux1)\n",
    "            \n",
    "            for columna in df_nuevo.columns:\n",
    "                for variables in list(self.dico.columns):\n",
    "                    if variables in columna:\n",
    "                        aux.append(columna)\n",
    "                    \n",
    "            self.dummy=df_nuevo[aux]\n",
    "            self[aux]=df_nuevo[aux]\n",
    "\n",
    "            # self.df=self.drop(columns=var, axis='columns')\n",
    "            # self.df= self[self.columns.difference(self.dico.columns)]\n",
    "            \n",
    "            print(\"********************** self.dummy ************\\n\")\n",
    "            print(self.dummy)\n",
    "            print(\"\\n********************** self.df o todas_las_col() ************\\n\")\n",
    "            print(self.df)\n",
    "\n",
    "        except:\n",
    "            print(\"---------------------- ERROR -----------------\")\n",
    "\n",
    "\n",
    "\n",
    "    def limpiar_dummys(self):\n",
    "\n",
    "        b=False\n",
    "        lista=list(self.dico.columns)\n",
    "        for ind, i in enumerate(lista):\n",
    "                if (ind+1<len(lista)):\n",
    "                    if( (i in lista [ind+1]) ):\n",
    "                        b=True\n",
    "                        break\n",
    "        if b:\n",
    "            nombres_nuevos=[]\n",
    "            if len(lista)>2:\n",
    "                for ind, i in enumerate(lista):\n",
    "                    if (ind+1<len(lista)):\n",
    "                        if( (i in lista [ind+1]) ):\n",
    "                            nombres_nuevos.append(i.upper())\n",
    "                        else:\n",
    "                            nombres_nuevos.append(i)\n",
    "                    else:\n",
    "                        nombres_nuevos.append(i)\n",
    "                        \n",
    "            aux_df=self.df\n",
    "\n",
    "            for i,j in zip(lista,nombres_nuevos):\n",
    "                aux_df.rename(columns={i:j},inplace=True)\n",
    "                \n",
    "            self.df=aux_df\n",
    "            self.dico.columns=nombres_nuevos\n",
    "            \n",
    "            self.limpiar_aux()\n",
    "        else: \n",
    "            self.limpiar_aux()\n",
    "\n",
    "\n",
    "\n",
    "    def estadistica_descriptiva_cuantis(self):\n",
    "\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\nDESCRIPCIÓN\")\n",
    "        print (self.cuanti.describe())\n",
    "        print(\"\\n\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\nCUARTILES\")\n",
    "        print (self.cuanti.quantile([0.05,0.25,0.5,0.75,0.95]))\n",
    "        print(\"\\n\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")\n",
    "        print(\"\\n\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "        aux1=self.dico.columns\n",
    "        aux2=self.cate.columns\n",
    "        aux=self.cuanti.columns\n",
    "\n",
    "        # df_auxiliar = self.groupby('sexo').apply(lambda x: pd.Series(shapiro(x), index=['W','P'])).reset_index()\n",
    "        # print(df_auxiliar)\n",
    "                \n",
    "        for a in list(aux1.values):\n",
    "            \n",
    "            for b in list(aux.values):\n",
    "                \n",
    "                print(\"++++++++++++++++++++++++++++  \"+a+\" y \"+b+\"  ++++++++++++++++++++++++++\\n\")\n",
    "                agrupado=self.groupby(a)[b]\n",
    "                titulo=f\"Agrupado por {a} y por {b}\"\n",
    "                print(titulo)\n",
    "                print(agrupado.describe().reset_index())\n",
    "                # df.groupby(['cat1', 'cat2'])['purchases','sales'].apply(stats.shapiro)\n",
    "                print(\"////////////////////////// TEST DE SHAPIRO ////////////////////////////\")\n",
    "                aux_shapiro=(agrupado.apply(stats.shapiro))\n",
    "                print(aux_shapiro)\n",
    "        \n",
    "                \n",
    "                print(\"\\n\")\n",
    "                print(\"----------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "    def estadistica_descriptiva_cualis(self):\n",
    "\n",
    "        print(\"\\n--------------------- Variables dico ---------------------\")\n",
    "        print(\"\\n\")\n",
    "        for i in self.dico.columns:\n",
    "            print(f\"...........Frecuencia variable {i} ....................\")\n",
    "            print(self[i].value_counts()/(self[i].count()))\n",
    "            print(\"\\n\")\n",
    "\n",
    "        print(\"\\n-------------------- Variables categoricas --------------------\")\n",
    "        print(\"\\n\")\n",
    "        for i in self.cate.columns:\n",
    "            print(f\"...........Frecuencia variable {i} ....................\")\n",
    "            print(self[i].value_counts()/(self[i].count()))\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        # crosstab variables cualis con cate\n",
    "        aux=list(self.cate.columns)\n",
    "\n",
    "        a=0\n",
    "        for i in aux:\n",
    "            a=a+1\n",
    "            if a<len(aux)/2:\n",
    "                b=0\n",
    "                for j in aux[:-1]:\n",
    "                    b=b+1\n",
    "                    if b > a:\n",
    "                        print(f\"*************** TABAL DE VARIABLES CATEGORICAS {i} y {j} *********************\\n \")\n",
    "                        tab = pd.crosstab (index=self[i], columns=self[j])\n",
    "                        x=(tab/tab.sum())\n",
    "                        print(tab)\n",
    "                        print(\"\\n\")\n",
    "                        print(f\"/////////////////// EN PROPORCION //////////////////\\n\")\n",
    "                        print(x)\n",
    "                        print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "    def anova(self):\n",
    "\n",
    "        aux_cate=list(self.cate.columns)\n",
    "        aux_cuati=list(self.cuanti.columns)\n",
    "\n",
    "        for i in aux_cate:\n",
    "            for j in aux_cuati:\n",
    "                try:\n",
    "                    print(f\"\\n----------- ANOVA Categoria {i} y variable continua {j} ----------\\n\")\n",
    "                    model = ols(f\"{j} ~ {i}\", data=self).fit()\n",
    "                    a=sm.stats.anova_lm(model, typ=2)\n",
    "                    print(a)\n",
    "                except:\n",
    "                    print(f\"\\n - - - - - Fallo en variable {i} y {j} - - - - - - \\n\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def Chi(self):\n",
    "\n",
    "        aux_dico=list(self.dico.columns)\n",
    "\n",
    "        if len(aux_dico)>1:\n",
    "            for ind, i in enumerate(aux_dico):\n",
    "                for j in range(ind+1,len(aux_dico)):\n",
    "                    chi, p, dof, expected = stats.chi2_contingency(pd.crosstab(self[i],self[aux_dico[j]]), correction=False)\n",
    "                    print(f\"\\n-------------- Chi2 entre {i} y {aux_dico[j]} ----------------\")\n",
    "                    print(f\"p: {p} \\n\") \n",
    "        else:\n",
    "            print(\"******************** No suficientes argumentos ********************\")\n",
    "\n",
    "\n",
    "    def t_test_aux(self, columns):\n",
    "        results = []\n",
    "        for i, col1 in enumerate(columns[:-1]):\n",
    "            for col2 in columns[i+1:]:\n",
    "                t, p = stats.ttest_ind(self[col1].dropna(), self[col2].dropna(), equal_var=False)\n",
    "                # results.append((col1, col2, t, p))\n",
    "                if p < 0.05:\n",
    "                    print( f\"+++++ Variable{col1}, variable 2 {col2} con p de: \\033[1m{p}\\033[0m  Se RECHAZA H0 ++++\") \n",
    "                else:\n",
    "                    print( f\"+++++ Variable{col1}, variable 2 {col2}  con p de: {p} SE ACEPTA H0 ++++\") \n",
    "    \n",
    "    def wilcoxon_test_aux(self,col1, col2):\n",
    "        if (col1== col2).all():\n",
    "            print (\"\\nLas coluimnas son iguales\\n\")\n",
    "        res = stats.wilcoxon(col1, col2)\n",
    "        if res.pvalue < 0.05:\n",
    "            print(f\"Reject null hypothesis. Significant difference  (p-value={res.pvalue:.4f})\")\n",
    "        else:\n",
    "            print (f\"Fail to reject null hypothesis. No significant difference (p-value={res.pvalue:.4f})\")\n",
    "\n",
    "    def wilconxon(self, lista):\n",
    "        # lista=[grupo, var]\n",
    "        a,b=self.agrupar(lista)\n",
    "        print(f\"\\n- Variable: {lista[1]}, Grupo: {lista[0]}\")\n",
    "        self.wilcoxon_test_aux(a, b)\n",
    "\n",
    "    def agrupar (self, lista):\n",
    "        groupby_col=lista[0]\n",
    "        col=lista[1]\n",
    "        valor=self[groupby_col].unique()\n",
    "        group= self.where(self[groupby_col]== valor[0])[col]\n",
    "        group2= self.where(self[groupby_col]== valor[1])[col] \n",
    "        return group,group2\n",
    "\n",
    "    # def t_test_groupby_one_col(self, col, groupby_col):\n",
    "        \n",
    "    #     group= self.where(self[groupby_col]== self[groupby_col][0]).dropna()[col]\n",
    "    #     group2= self.where(self[groupby_col]== self[groupby_col][1]).dropna()[col]\n",
    "    #     t, p = stats.ttest_ind(group, group2, equal_var=False)\n",
    "    #     print( col, groupby_col,p) \n",
    "\n",
    "    def t_test_all(self):\n",
    "        aux=list(self.cuanti.columns)\n",
    "        aux2=list(self.dico.columns)\n",
    "        self.t_test_aux(self.normal_cuatis) #aqui ya hace todas las cuantis entre ellas faltan los grupos\n",
    "        for i in self.normal_grupos_dico:\n",
    "            a,b=self.agrupar(i)\n",
    "            t, p = stats.ttest_ind(a.dropna(), b.dropna(), equal_var=False)\n",
    "            if p < 0.05:\n",
    "                print( f\"+++++ Variable{i[1]}, Agrupado por {i[0]} con p de: \\033[1m{p}\\033[0m  Se RECHAZA H0 ++++\") \n",
    "            else:\n",
    "                print( f\"+++++ Variable{i[1]}, Agrupado por {i[0]} con p de: {p} SE ACEPTA H0 ++++\") \n",
    "    # df_prueba.groupby('sexo').apply(lambda df: stats.ttest_ind(df['Datos_D'].dropna(), df['Datos_E'].dropna())[1])\n",
    "\n",
    "\n",
    "    def plot_confidence_interval(self, col, confidence_level= 0.95):\n",
    "        data = self[col].to_numpy()\n",
    "        n = len(data)\n",
    "        mean =self[col].mean(axis=0)\n",
    "        # std_error = stats.sem( self[col].dropna())\n",
    "        std_error = self[col].dropna().std()\n",
    "        lower_bound = stats.t.ppf(0.025, n - 1, loc = mean, scale = std_error)  # =>  99.23452406698323\n",
    "        upper_bound = stats.t.ppf(0.975, n - 1, loc = mean, scale = std_error)\n",
    "        # h = std_error * stats.t.ppf((1 + confidence_level) / 2, n - 1)\n",
    "        \n",
    "        # lower_bound = mean - h\n",
    "        # upper_bound = mean + h\n",
    "        # plt.hist(data, bins=30, edgecolor='black', alpha=0.5)\n",
    "        # plt.axvspan(lower_bound, upper_bound, color='gray', alpha=0.2, label=f'{confidence_level * 100}% Confidence Interval')\n",
    "        # plt.axvline(x=mean, color='red', label='Sample Mean')\n",
    "        # plt.legend()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(data, bins=30, edgecolor='black', alpha=0.5)\n",
    "        ax.axvline(x=mean, color='red', label='Sample Mean')\n",
    "        ax.axvspan(lower_bound, upper_bound, color='grey', alpha=0.5, label=f'{confidence_level * 100}% Confidence Interval')\n",
    "        ax.annotate(\n",
    "            f'lower_bound:\\n {lower_bound:.2f}',\n",
    "            xy=(lower_bound, 0), xytext=(lower_bound-0.5, 50)\n",
    "        )\n",
    "        ax.annotate(\n",
    "            f'upper_bound:\\n  {upper_bound:.2f}',\n",
    "            xy=(upper_bound, 0), xytext=(upper_bound-0.5, 50)\n",
    "        )\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_normailidad(self):\n",
    "        aux=self.cuanti.columns\n",
    "        for i in aux:\n",
    "            stats.probplot(self[i], dist=\"norm\", plot=plt)\n",
    "            plt.title(\"Probability Plot - \" )\n",
    "            plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "    def plot_bigotes(self):\n",
    "\n",
    "        aux1=self.dico.columns\n",
    "        aux2=self.cate.columns\n",
    "        aux=self.cuanti.columns\n",
    "        print(\"\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "\n",
    "        print(\"-------------- Graficas de bigotes cualitativas-------------------\")\n",
    "        # fig = plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        (self.cuanti).plot(kind='box', title='Variables cuantitativas',figsize=(12, 8))\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        print(\"-------------- Graficas de bigotes por dicotomicas-------------------\")   \n",
    "        \n",
    "        for a in aux1:\n",
    "\n",
    "            # fig = plt.figure(figsize=(12, 8))\n",
    "            self.boxplot(column=list(aux.values), by=a,figsize=(12, 8))\n",
    "            plt.tight_layout() \n",
    "            plt.show()\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "        print(\"-------------- Graficas de bigotes por categoricas-------------------\") \n",
    "\n",
    "        for a in aux2:\n",
    "            # fig = plt.figure(figsize=(12, 8))\n",
    "            ax= self.boxplot(column=list(aux.values), by=a, figsize=(12, 8))\n",
    "            # ax = sns.swarmplot(column=list(aux.values), by=a,data=self, color='#7d0013')\n",
    "            plt.tight_layout() \n",
    "            plt.show()\n",
    "        print(\"\\n\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        \n",
    "        print(\"\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    def plot_corr(self):\n",
    "\n",
    "        print(\"\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")\n",
    "        \n",
    "        print(\"-------------- MATRIZ DE CORRELACIONES ENTRE CUANTITATIVAS -------------------\\n\") \n",
    "\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        matrix = self.cuanti.corr().round(2)\n",
    "        mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "        sns.heatmap(matrix, annot=True, vmax=1, vmin=-1, center=0, cmap='vlag', mask=mask)  \n",
    "        plt.show()\n",
    "\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        print(\"\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "\n",
    "\n",
    "    def plot_barras(self):\n",
    "        aux=self.cuanti.columns\n",
    "\n",
    "        print(\"\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        print(\"-------------- GRAFICA DE BARRAS DE TODAS LAS CUANTITATIVAS -------------------\\n\") \n",
    "        # fig = plt.figure(figsize=(15, 20))\n",
    "        self.cuanti.plot.bar(figsize=(18, 8))\n",
    "        plt.show()\n",
    "\n",
    "        print(\"-------------- GRAFICA DE BARRAS CON DISTRIBUCIÓN DE DENSIDAD DE CADA CUANTITATIVA  -------------------\\n\") \n",
    "        for i in list(aux.values):\n",
    "            fig = plt.figure(figsize=(12, 8))\n",
    "            print(f\"\\n.............. GRAFICA DE BARRAS  DE {i} ............\\n\") \n",
    "            ax=self[i].plot.hist(density=True)\n",
    "            self[i].plot.density(ax=ax)\n",
    "            plt.show()\n",
    "\n",
    "        print(\"----------------------------------------------------------------------------------------------------\\n\")    \n",
    "        print(\"\\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "\n",
    "\n",
    "    def todos_plots(self):\n",
    "\n",
    "        self.plot_bigotes()\n",
    "        self.plot_corr()\n",
    "        self.plot_barras()\n",
    "        self.violines()\n",
    "        \n",
    "        \n",
    "\n",
    "    def violines(self):\n",
    "\n",
    "        aux1=self.dico.columns\n",
    "        aux2=self.cate.columns\n",
    "        aux=self.cuanti.columns\n",
    "\n",
    "        print(\"--------------  GRAFICA DE VIOLINES  -------------------\\n\") \n",
    "        sns.set(style=\"whitegrid\")\n",
    "        for i in aux2:\n",
    "            for j in aux:\n",
    "                ax= sns.violinplot(x=self[i], y=self[j], palette=\"Set2\", split=True, inner=\"quartile\",scale=\"count\")\n",
    "                plt.show()\n",
    "\n",
    "        print(\"\\n\\n/////////-------------- GRAFICA DE VIOLINES POR DICOTOMICAS -------------------/////////////\\n\") \n",
    "        \n",
    "        for i in aux2:\n",
    "            for j in aux:\n",
    "                for k in aux1:\n",
    "                    ax= sns.violinplot(x=self[i], y=self[j], hue=self[k],palette=\"Set2\", split=True, inner=\"quartile\",scale=\"count\")\n",
    "                    plt.show()\n",
    "\n",
    "\n",
    "    def cross_var_cualis_con_ciantis(self):\n",
    "\n",
    "        aux=list(self.cate.columns)\n",
    "        aux_cuati=list(self.cuanti.columns)\n",
    "\n",
    "        for k in aux_cuati:\n",
    "            a=0\n",
    "            for i in aux:\n",
    "                a=a+1\n",
    "                if a<len(aux)/2:\n",
    "                    b=0\n",
    "                    for j in aux[:-1]:\n",
    "                        b=b+1\n",
    "                        if b > a:\n",
    "                            print(f\"\\n\\n*************** TABAL DE VARIABLES CATEGORICAS {i} y {j} con valores de {k} MEDIA *********************\\n \")\n",
    "                            tab = pd.crosstab (index=self[i], columns=self[j],values=self[k],aggfunc=np.mean)\n",
    "                            print(tab)\n",
    "                            print(\"\\n\\n\")\n",
    "\n",
    "    def nulos(self):\n",
    "        aux_df=list(self.cuanti.columns)\n",
    "        aux_DF=self.cuanti\n",
    "        self.df_inputado=self.df\n",
    "        for i in aux_df:\n",
    "            nulos=aux_DF[i].isna().sum()\n",
    "            total=len(aux_DF[i])\n",
    "            porcentaje=nulos/total\n",
    "            if ((nulos>0) and (porcentaje<self.porcentaje_nulos_permitido)):\n",
    "                percen=aux_DF[i].quantile([0.2,0.8]).to_list()\n",
    "                self.df_inputado[i]=aux_DF[i].apply(lambda x: ( random.randint ( round(percen[0]) , round(percen[1]) )) if pd.isna(x) else x )\n",
    "                print(f\"\\n- Se han inputado {nulos} nulos a la variable {i} (tenía porcentaje de nulos de: {porcentaje}) \\n\")\n",
    "            elif (porcentaje>self.porcentaje_nulos_permitido):\n",
    "                print(f\"\\n - No se ha podido inputar a la variable {i} porque el porcentaje de nulos era de {porcentaje}\\n\")\n",
    "                \n",
    "\n",
    "    def normalidad(self):\n",
    "        \n",
    "        DataF=self.df\n",
    "        aux1=self.dico.columns\n",
    "        aux2=self.cate.columns\n",
    "        aux=self.cuanti.columns\n",
    "                \n",
    "        for b in list(aux.values):\n",
    "            aux_shapiro=(stats.shapiro(DataF[b]))\n",
    "            if(aux_shapiro.pvalue<0.05):\n",
    "                print(\"////////////////////////// TEST DE SHAPIRO CUANTITATIVAS ////////////////////////////\")\n",
    "                print(\"++++++++++++++++++++++++++++  \"+ b +\"  ++++++++++++++++++++++++++\\n\")\n",
    "                titulo=f\"Variable cuantitativa {b} y test Shapiro < 0.05\"\n",
    "                print(titulo)\n",
    "                print(aux_shapiro)\n",
    "                print(\"\\n\")\n",
    "                print(\"----------------------------------------------------------------------------------------------------\\n\")\n",
    "                self.normal_cuatis.append(b)\n",
    "\n",
    "        for a in list(aux1.values):\n",
    "            for b in list(aux.values):\n",
    "                    agrupado=DataF.groupby(a)[b]\n",
    "                    try:\n",
    "                        aux_shapiro=(agrupado.apply(stats.shapiro))\n",
    "                        for h in aux_shapiro:\n",
    "                            if(h.pvalue<0.05):\n",
    "                                print(\"////////////////////////// TEST DE SHAPIRO DICOTOMICAS ////////////////////////////\")\n",
    "                                print(\"++++++++++++++++++++++++++++  \"+a+\" y \"+b+\"  ++++++++++++++++++++++++++\\n\")\n",
    "                                titulo=f\"Agrupado por {a} y por {b} y test Shapiro < 0.05\"\n",
    "                                print(titulo)\n",
    "                                print(aux_shapiro)\n",
    "                                print(\"\\n\")\n",
    "                                print(\"----------------------------------------------------------------------------------------------------\\n\")\n",
    "                                self.normal_grupos_dico.append([a,b])\n",
    "                    except:\n",
    "                        continue \n",
    "\n",
    "        for a in list(aux2.values):\n",
    "            for b in list(aux.values):\n",
    "                    agrupado=DataF.groupby([a])[b]\n",
    "                    try:\n",
    "                        aux_shapiro=(agrupado.apply(stats.shapiro))\n",
    "                        for h in aux_shapiro:\n",
    "                            if(h.pvalue<0.05):\n",
    "                                print(\"////////////////////////// TEST DE SHAPIRO CATEGORICAS ////////////////////////////\")\n",
    "                                print(\"++++++++++++++++++++++++++++  \"+a+\" y \"+b+\"  ++++++++++++++++++++++++++\\n\")\n",
    "                                titulo=f\"Agrupado por {a} y por {b} y test Shapiro < 0.05\"\n",
    "                                print(titulo)\n",
    "                                print(h)\n",
    "                                print(\"\\n\")\n",
    "                                print(\"----------------------------------------------------------------------------------------------------\\n\")\n",
    "                                self.normal_grupos_cate.append([a,b])\n",
    "                    except:\n",
    "                        continue \n",
    "\n",
    "        self.normal_grupos_dico=[i for n, i in enumerate(self.normal_grupos_dico) if i not in self.normal_grupos_dico[:n]]\n",
    "        self.normal_grupos_cate=[i for n, i in enumerate(self.normal_grupos_cate) if i not in self.normal_grupos_cate[:n]]\n",
    "        \n",
    "    def detec_outlaiers(self):\n",
    "        aux=list(self.cuanti.columns)\n",
    "        aux_DF=self.cuanti\n",
    "        for i in aux:\n",
    "            z = np.abs(stats.zscore(aux_DF[i]))\n",
    "            print(z)\n",
    "    \n",
    "    def seleccionar_distribuciones(self,familia='realall', verbose=False):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        familia : {'realall', 'realline', 'realplus', 'real0to1', 'discreta'}\n",
    "            realall: distribuciones de la familia `realline` + `realplus`\n",
    "            realline: distribuciones continuas en el dominio (-inf, +inf)\n",
    "            realplus: distribuciones continuas en el dominio [0, +inf)\n",
    "            real0to1: distribuciones continuas en el dominio [0,1]\n",
    "            discreta: distribuciones discretas\n",
    "            \n",
    "        verbose : bool\n",
    "            Si se muestra información de las distribuciones seleccionadas\n",
    "            (the default `False`)\n",
    "        '''\n",
    "    \n",
    "        distribuciones = [getattr(stats,d) for d in dir(stats) \\\n",
    "                        if isinstance(getattr(stats,d), (stats.rv_continuous, stats.rv_discrete))]\n",
    "        \n",
    "        exclusiones = ['levy_stable', 'vonmises']\n",
    "        distribuciones = [dist for dist in distribuciones if dist.name not in exclusiones]\n",
    "                \n",
    "        dominios = {\n",
    "            'realall' : [-np.inf, np.inf],\n",
    "            'realline': [np.inf,np.inf],\n",
    "            'realplus': [0, np.inf],\n",
    "            'real0to1': [0, 1], \n",
    "            'discreta': [None, None],\n",
    "        }\n",
    "\n",
    "        distribucion = []\n",
    "        tipo = []\n",
    "        dominio_inf = []\n",
    "        dominio_sup = []\n",
    "\n",
    "        for dist in distribuciones:\n",
    "            distribucion.append(dist.name)\n",
    "            tipo.append(np.where(isinstance(dist, stats.rv_continuous), 'continua', 'discreta'))\n",
    "            dominio_inf.append(dist.a)\n",
    "            dominio_sup.append(dist.b)\n",
    "        \n",
    "        info_distribuciones = pd.DataFrame({\n",
    "                                'distribucion': distribucion,\n",
    "                                'tipo': tipo,\n",
    "                                'dominio_inf': dominio_inf,\n",
    "                                'dominio_sup': dominio_sup\n",
    "                            })\n",
    "\n",
    "        info_distribuciones = info_distribuciones \\\n",
    "                            .sort_values(by=['dominio_inf', 'dominio_sup'])\\\n",
    "                            .reset_index(drop=True)\n",
    "        \n",
    "        if familia in ['realall', 'realline', 'realplus', 'real0to1']:\n",
    "            info_distribuciones = info_distribuciones[info_distribuciones['tipo']=='continua']\n",
    "            condicion = (info_distribuciones['dominio_inf'] == dominios[familia][0]) & \\\n",
    "                        (info_distribuciones['dominio_sup'] == dominios[familia][1]) \n",
    "            info_distribuciones = info_distribuciones[condicion].reset_index(drop=True)\n",
    "            \n",
    "        if familia in ['discreta']:\n",
    "            info_distribuciones = info_distribuciones[info_distribuciones['tipo']=='discreta']\n",
    "            \n",
    "        seleccion = [dist for dist in distribuciones \\\n",
    "                    if dist.name in info_distribuciones['distribucion'].values]\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(\"---------------------------------------------------\")\n",
    "            print(\"       Distribuciones seleccionadas                \")\n",
    "            print(\"---------------------------------------------------\")\n",
    "            with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n",
    "                print(info_distribuciones)\n",
    "        \n",
    "        return seleccion\n",
    "\n",
    "\n",
    "    def plot_multiple_distribuciones(self, nombre_distribuciones):\n",
    "\n",
    "        aux=list(self.cuanti.columns)\n",
    "        fig, ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "        for i in aux:\n",
    "            x=self[i]\n",
    "            if ax is None:\n",
    "                fig, ax = plt.subplots(figsize=(7,4))\n",
    "                \n",
    "            ax.hist(x=x, density=True, bins=30, color=\"#3182bd\", alpha=0.5)\n",
    "            ax.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)\n",
    "            ax.set_title('Ajuste distribuciones')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('Densidad de probabilidad')\n",
    "            \n",
    "            for nombre in nombre_distribuciones:\n",
    "                \n",
    "                distribucion = getattr(stats, nombre)\n",
    "\n",
    "                parametros = distribucion.fit(data=x)\n",
    "\n",
    "                nombre_parametros = [p for p in inspect.signature(distribucion._pdf).parameters \\\n",
    "                                    if not p=='x'] + [\"loc\",\"scale\"]\n",
    "                parametros_dict = dict(zip(nombre_parametros, parametros))\n",
    "\n",
    "                log_likelihood = distribucion.logpdf(x, *parametros).sum()\n",
    "\n",
    "                aic = -2 * log_likelihood + 2 * len(parametros)\n",
    "                bic = -2 * log_likelihood + np.log(x.shape[0]) * len(parametros)\n",
    "\n",
    "                x_hat = np.linspace(min(x), max(x), num=100)\n",
    "                y_hat = distribucion.pdf(x_hat, *parametros)\n",
    "                ax.plot(x_hat, y_hat, linewidth=2, label=distribucion.name)\n",
    "            \n",
    "            ax.legend()\n",
    "            plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "    def fit_discrete(self,datos):\n",
    "\n",
    "        # self.discreta\n",
    "\n",
    "        mean = datos.mean()\n",
    "        var = datos.var()\n",
    "        likelihoods = {}  \n",
    "        log_likelihoods = {}\n",
    "\n",
    "        p = 1 - mean / var  \n",
    "        r = (1-p) * mean / p\n",
    "\n",
    "\n",
    "\n",
    "        log_likelihoods['nbinom'] = datos.map(lambda val: stats.nbinom.logpmf(val, r, p)).sum()\n",
    "\n",
    "        lambda_ = mean\n",
    "\n",
    "        log_likelihoods['poisson'] = datos.map(lambda val: stats.poisson.logpmf(val, lambda_)).sum()\n",
    "\n",
    "\n",
    "        best_fit = max(log_likelihoods, key=lambda x: log_likelihoods[x])\n",
    "        print(\"**** Best fit between poisson and nbinorm :\", best_fit)\n",
    "        \n",
    "\n",
    "    \n",
    "        plt.hist(datos, bins=int(np.max(datos)), density=True, alpha=0.5)\n",
    "\n",
    "        mean = datos.mean()\n",
    "        var = datos.var()\n",
    "\n",
    "\n",
    "        def loss_function_poisson(params, datos_in):\n",
    "\n",
    "            mu = params[0]\n",
    "\n",
    "            loss = 0\n",
    "\n",
    "            for i in range(len(datos_in)):\n",
    "\n",
    "                loglikelihood = stats.poisson.logpmf(datos_in[i], mu)\n",
    "\n",
    "                loss_to_add = -loglikelihood\n",
    "\n",
    "                loss += loss_to_add\n",
    "\n",
    "            return(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        params0 = np.array([20])\n",
    "        minimum = stats2.optimize.fmin(loss_function_poisson, params0, args=(datos,))\n",
    "\n",
    "        mu_fit = minimum[0]\n",
    "\n",
    "        print(\"***********  The best mu_fit is:  \",  mu_fit)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        x = list(range(int(np.min(datos)), int(np.max(datos))+1))\n",
    "        plt.scatter(x, stats.poisson.pmf(x, mu_fit),color=\"red\")\n",
    "        plt.show()   \n",
    "\n",
    "        print(\"\\n\\n Otras variables discretas:  \",  self.discreta)\n",
    "\n",
    "\n",
    "    def comparar_distribuciones_caunti_cont(self, ordenar='aic', verbose=False):\n",
    "\n",
    "            '''\n",
    "            resultados: data.frame\n",
    "                distribucion: nombre de la distribución.\n",
    "                log_likelihood: logaritmo del likelihood del ajuste.\n",
    "                aic: métrica AIC.\n",
    "                bic: métrica BIC.\n",
    "                n_parametros: número de parámetros de la distribución de la distribución.\n",
    "                parametros: parámetros del tras el ajuste\n",
    "                \n",
    "            Raises\n",
    "            ------\n",
    "            Exception\n",
    "                Si `familia` es distinto de 'realall', 'realline', 'realplus', 'real0to1',\n",
    "                o 'discreta'.\n",
    "                \n",
    "            Notes\n",
    "            -----\n",
    "            '''\n",
    "            aux=list(self.cuanti.columns)\n",
    "            \n",
    "            for i in aux:\n",
    "                print(f\"\\n ******************** Variable: {i} ******************** \\n\")\n",
    "                x=self[i]\n",
    "                distribuciones = self.seleccionar_distribuciones(familia='realall',verbose=verbose)\n",
    "                distribucion_ = []\n",
    "                log_likelihood_= []\n",
    "                aic_ = []\n",
    "                bic_ = []\n",
    "                n_parametros_ = []\n",
    "                parametros_ = []\n",
    "                \n",
    "                for j, distribucion in enumerate(distribuciones):\n",
    "                    \n",
    "                    # print(f\"{j+1}/{len(distribuciones)} Ajustando distribución: {distribucion.name}\")\n",
    "                    \n",
    "                    try:\n",
    "                        parametros = distribucion.fit(data=x)\n",
    "                        nombre_parametros = [p for p in inspect.signature(distribucion._pdf).parameters \\\n",
    "                                            if not p=='x'] + [\"loc\",\"scale\"]\n",
    "                        parametros_dict = dict(zip(nombre_parametros, parametros))\n",
    "                        log_likelihood = distribucion.logpdf(x, *parametros).sum()\n",
    "                        aic = -2 * log_likelihood + 2 * len(parametros)\n",
    "                        bic = -2 * log_likelihood + np.log(x.shape[0]) * len(parametros)\n",
    "                        \n",
    "                        distribucion_.append(distribucion.name)\n",
    "                        log_likelihood_.append(log_likelihood)\n",
    "                        aic_.append(aic)\n",
    "                        bic_.append(bic)\n",
    "                        n_parametros_.append(len(parametros))\n",
    "                        parametros_.append(parametros_dict)\n",
    "                        \n",
    "                        resultados = pd.DataFrame({\n",
    "                                        'distribucion': distribucion_,\n",
    "                                        'log_likelihood': log_likelihood_,\n",
    "                                        'aic': aic_,\n",
    "                                        'bic': bic_,\n",
    "                                        'n_parametros': n_parametros_,\n",
    "                                        'parametros': parametros_,\n",
    "                            \n",
    "                                    })\n",
    "                        \n",
    "                        resultados = resultados.sort_values(by=ordenar).reset_index(drop=True)\n",
    "\n",
    "                        \n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error al tratar de ajustar la distribución {distribucion.name}\")\n",
    "                        print(e)\n",
    "                        print(\"\")\n",
    "\n",
    "                nombre_distribuciones=resultados['distribucion'][:5]\n",
    "                fig, ax = plt.subplots(figsize=(7,4))\n",
    "                \n",
    "                \n",
    "                ax.hist(x=x, density=True, bins=30, color=\"#3182bd\", alpha=0.5)\n",
    "                ax.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)\n",
    "                ax.set_title('Ajuste distribuciones')\n",
    "                ax.set_xlabel('x')\n",
    "                ax.set_ylabel('Densidad de probabilidad')\n",
    "                \n",
    "                for nombre in nombre_distribuciones:\n",
    "                    \n",
    "                    distribucion = getattr(stats, nombre)\n",
    "\n",
    "                    parametros = distribucion.fit(data=x)\n",
    "\n",
    "                    nombre_parametros = [p for p in inspect.signature(distribucion._pdf).parameters \\\n",
    "                                        if not p=='x'] + [\"loc\",\"scale\"]\n",
    "                    parametros_dict = dict(zip(nombre_parametros, parametros))\n",
    "\n",
    "                    log_likelihood = distribucion.logpdf(x, *parametros).sum()\n",
    "\n",
    "                    aic = -2 * log_likelihood + 2 * len(parametros)\n",
    "                    bic = -2 * log_likelihood + np.log(x.shape[0]) * len(parametros)\n",
    "\n",
    "                    x_hat = np.linspace(min(x), max(x), num=100)\n",
    "                    y_hat = distribucion.pdf(x_hat, *parametros)\n",
    "                    ax.plot(x_hat, y_hat, linewidth=2, label=distribucion.name)\n",
    "            \n",
    "                ax.legend()\n",
    "                plt.show()\n",
    "\n",
    "                print(\"\\n\")\n",
    "                print(resultados.head(5))    \n",
    "                print(\"\\n------------------------------------------------------------------\\n\")\n",
    "\n",
    "    def remove_outliers(self, k=1.5):\n",
    "        aux=list(self.cuanti.columns)\n",
    "        for column in aux:\n",
    "            print(f\"\\n\\n                    <<<<<<<<<<<<<<<<<<<<<<<< {column} >>>>>>>>>>>>>>>>>>>>>>>>\\n\\n\")\n",
    "            self.plot_outliers2(column, k=1.5)\n",
    "            q1, q3 = self[column].quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - (k * iqr)\n",
    "            upper_bound = q3 + (k * iqr)\n",
    "            self.loc[(self[column] < lower_bound) | (self[column] > upper_bound), column] = None    \n",
    "        self.nulos()\n",
    "\n",
    "\n",
    "    def plot_outliers(self, column, k=1.5):\n",
    "        \n",
    "        q1, q3 = self[column].quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (k * iqr)\n",
    "        upper_bound = q3 + (k * iqr)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(self.index, self[column], color='blue', label='inlier')\n",
    "        ax.scatter(self[(self[column] < lower_bound) | (self[column] > upper_bound)].index,\n",
    "                self[(self[column] < lower_bound) | (self[column] > upper_bound)][column],\n",
    "                color='red', label='outlier')\n",
    "        ax.axhline(lower_bound, color='gray', linestyle='--')\n",
    "        ax.axhline(upper_bound, color='gray', linestyle='--')\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "\n",
    "    \n",
    "    def plot_outliers2(df, column, k=1.5):\n",
    "        q1, q3 = df[column].quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (k * iqr)\n",
    "        upper_bound = q3 + (k * iqr)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(df.index, df[column], color='blue')\n",
    "        ax.scatter(df[df[column].isnull()].index,\n",
    "                df[df[column].isnull()][column],\n",
    "                color='red', marker='x')\n",
    "        ax.axhline(lower_bound, color='red', linestyle='--')\n",
    "        ax.axhline(upper_bound, color='red', linestyle='--')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_xy_data(df, x_column, y_column):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(df[x_column], df[y_column])\n",
    "        plt.xlabel(x_column)\n",
    "        plt.ylabel(y_column)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREACIÓN DE LA CLASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo=DF_exploracion(df_prueba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas._libs.properties.AxisProperty at 0x250ae5f52a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ejemplo.cuanti.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINICIÓN DE LAS VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.variables()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de variables dummys a traves de dicotómicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.limpiar_dummys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisión de todas las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplo.df\n",
    "# ejemplo.cuanti\n",
    "# ejemplo.dummy\n",
    "# ejemplo.dico\n",
    "ejemplo.df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisión de variables agrupadas automaticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Datos con distribución discreta: []\n",
      "Datos de tipos string seguramente: []\n"
     ]
    }
   ],
   "source": [
    "print(f\" Datos con distribución discreta: {ejemplo.discreta}\")\n",
    "print(f\"Datos de tipos string seguramente: {ejemplo.stingg}\")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESTADISTICA DESCRIPTIVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.estadistica_descriptiva_cuantis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.estadistica_descriptiva_cualis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.cross_var_cualis_con_ciantis()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables normales y no normales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.normalidad()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupación normal por categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ejemplo.normal_grupos_cate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupación normalidad por dicotomicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ejemplo.normal_grupos_dico"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quitar outlayers e inputar datos en columnas variables cuantitativas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.remove_outliers()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustar a distribuciones variables cuantitativas (No puede haber nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.comparar_distribuciones_caunti_cont()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustar a distribuciones variables discretas (No puede haber nulos) (Solo poisson y binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.fit_discrete(df_prueba[\"Datos_Poisson_1\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejemplo.todos_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.plot_bigotes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.plot_barras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.plot_corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.violines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.plot_normailidad()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST ESTADISTICOS NO MULTIVARIANTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables cualitativas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.Chi()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables cuantitativas no pareadas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.t_test_all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wilconxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.wilconxon( [\"sexo\",\"Datos_D\" ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.anova()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intervalos de confianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo.plot_confidence_interval(\"Datos_D\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series Temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos_D: 11.628344385226061\n",
      "Datos_E: 0.99892670230118\n",
      "Datos_F: -0.050195968714205175\n",
      "- RMSE: 288\n",
      "- R2: 0.0041\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Datos_G</td>     <th>  R-squared:         </th> <td>   0.004</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.001</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.352</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 11 Feb 2023</td> <th>  Prob (F-statistic):</th>  <td> 0.256</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:10:24</td>     <th>  Log-Likelihood:    </th> <td> -7081.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>1.417e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   996</td>      <th>  BIC:               </th> <td>1.419e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Datos_D</th> <td>   11.6283</td> <td>    9.252</td> <td>    1.257</td> <td> 0.209</td> <td>   -6.527</td> <td>   29.783</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Datos_E</th> <td>    0.9989</td> <td>    9.210</td> <td>    0.108</td> <td> 0.914</td> <td>  -17.074</td> <td>   19.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Datos_F</th> <td>   -0.0502</td> <td>    0.032</td> <td>   -1.583</td> <td> 0.114</td> <td>   -0.112</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>  515.8888</td> <td>   18.388</td> <td>   28.056</td> <td> 0.000</td> <td>  479.805</td> <td>  551.973</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>526.406</td> <th>  Durbin-Watson:     </th> <td>   1.970</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  56.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.078</td>  <th>  Prob(JB):          </th> <td>5.24e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.846</td>  <th>  Cond. No.          </th> <td>1.17e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.17e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                Datos_G   R-squared:                       0.004\n",
       "Model:                            OLS   Adj. R-squared:                  0.001\n",
       "Method:                 Least Squares   F-statistic:                     1.352\n",
       "Date:                Sat, 11 Feb 2023   Prob (F-statistic):              0.256\n",
       "Time:                        18:10:24   Log-Likelihood:                -7081.1\n",
       "No. Observations:                1000   AIC:                         1.417e+04\n",
       "Df Residuals:                     996   BIC:                         1.419e+04\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Datos_D       11.6283      9.252      1.257      0.209      -6.527      29.783\n",
       "Datos_E        0.9989      9.210      0.108      0.914     -17.074      19.072\n",
       "Datos_F       -0.0502      0.032     -1.583      0.114      -0.112       0.012\n",
       "const        515.8888     18.388     28.056      0.000     479.805     551.973\n",
       "==============================================================================\n",
       "Omnibus:                      526.406   Durbin-Watson:                   1.970\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               56.554\n",
       "Skew:                           0.078   Prob(JB):                     5.24e-13\n",
       "Kurtosis:                       1.846   Cond. No.                     1.17e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.17e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "predictores=['Datos_D','Datos_E','Datos_F']\n",
    "OUTCOME='Datos_G'\n",
    "#No funciona con NA ni con distinta longitud dentro de los DF\n",
    "\n",
    "ej_lm=LinearRegression()\n",
    "ej_lm.fit(df_prueba[predictores],df_prueba[OUTCOME])\n",
    "\n",
    "for name, coef in zip(predictores,ej_lm.coef_):\n",
    "    print(f\"{name}: {coef}\")\n",
    "\n",
    "fitted= ej_lm.predict(df_prueba[predictores])\n",
    "RMSE= np.sqrt(mean_squared_error(df_prueba[OUTCOME],fitted))\n",
    "r2= r2_score(df_prueba[OUTCOME],fitted) \n",
    "\n",
    "# RMSE es como el accuracy del modelo (es practicamente igual al RSE)\n",
    "print(f\"- RMSE: {RMSE:.0f}\")\n",
    "\n",
    "# coeficiente de determinación:  0-1 proporción de varianza en los datos\n",
    "# que estan contabilizados en el modelo\n",
    "print(f\"- R2: {r2:.4f}\")\n",
    "\n",
    "\n",
    "model=sm.OLS(df_prueba[OUTCOME],df_prueba[predictores].assign(const=1) )\n",
    "resul=model.fit()\n",
    "resul.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables: Datos_D, Datos_E, Datos_F\n",
      "Start: score=14170.29, constant\n",
      "Step: score=14169.82, add Datos_F\n",
      "Step: score=14169.82, unchanged None\n",
      "Intercept: 515.396\n",
      "Coeficients:\n",
      "Datos_F: -0.049726033290689885\n"
     ]
    }
   ],
   "source": [
    "from dmba import stepwise_selection,AIC_score\n",
    "\n",
    "y=df_prueba[OUTCOME]\n",
    "d=pd.get_dummies(df_prueba[predictores],drop_first=True)\n",
    "\n",
    "# var es un df de las vars\n",
    "def train_model(var):\n",
    "    if len(var)==0:\n",
    "        return None\n",
    "    model= LinearRegression()\n",
    "    model.fit(d[var],y)\n",
    "    return model \n",
    "\n",
    "def score_model(model,var):\n",
    "    if (len(var)==0):\n",
    "        return( AIC_score(y, [y.mean()]*len(y), model, df=1))\n",
    "    return(AIC_score(y,model.predict(d[var]), model) )\n",
    "\n",
    "best_model, best_var= stepwise_selection(d.columns, train_model, score_model, verbose=True)\n",
    "\n",
    "\n",
    "print(f\"Intercept: {best_model.intercept_:.3f}\")\n",
    "print(f\"Coeficients:\")\n",
    "for name, coef in zip(best_var,best_model.coef_):\n",
    "    print(f\"{name}: {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight regrassion\n",
    "#  - diferentes observaciones medidas con precisiones diferentes\n",
    "#  - Filas representas distintos casos\n",
    "\n",
    "def weighted_regression(df, outcome, predictors, weights):\n",
    "    X = df[predictors].values\n",
    "    y = df[outcome].values\n",
    "    w = df[weights].values\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y, sample_weight=w)\n",
    "\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datos_cate_C</th>\n",
       "      <th>Col_a_codificar_grupos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Datos_cate_C  Col_a_codificar_grupos\n",
       "0               8                       4\n",
       "1               7                       3\n",
       "2              38                       1\n",
       "3              45                       0\n",
       "4              30                       1\n",
       "..            ...                     ...\n",
       "995            43                       1\n",
       "996            13                       1\n",
       "997            33                       4\n",
       "998            12                       3\n",
       "999            22                       1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# si tienes variables con muchas categorias pues \n",
    "# tendrías un monton de dummys y no es la cosa tampoco \n",
    "# que va a parecer esto un sudoku (es que además pueden incluso ser practicamente iguales)\n",
    "# Entonces los puedes codificar usando los residuos de la regresión\n",
    "\n",
    "grupos1 = pd.DataFrame([\n",
    "    *pd.DataFrame ({\n",
    "    'Datos_cate_C': df_prueba['Datos_cate_C'],\n",
    "    'residuos':df_prueba[OUTCOME]-ej_lm.predict(df_prueba[predictores])\n",
    "    }).groupby(['Datos_cate_C']).apply(lambda x:\n",
    "       {\n",
    "    'Datos_cate_C' : x.iloc[0,0],\n",
    "    'count': len(x),\n",
    "    'residuo_medio': x.residuos.median()\n",
    "       })\n",
    "]).sort_values( 'residuo_medio')\n",
    "\n",
    "grupos1['cum_count']=np.cumsum(grupos1['count'])\n",
    "grupos1['Col_a_codificar_grupos']=pd.qcut(grupos1['cum_count'],5,labels=False,retbins=False)\n",
    "to_join= grupos1[['Datos_cate_C','Col_a_codificar_grupos']].set_index('Datos_cate_C')\n",
    "\n",
    "df_prueba=df_prueba.join(to_join, on='Datos_cate_C')\n",
    "df_prueba[['Datos_cate_C','Col_a_codificar_grupos']]\n",
    "# grupos4=grupos3.sort_values( 'residuo_medio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Datos_G</td>     <th>  R-squared:         </th> <td>   0.029</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.025</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   6.042</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 11 Feb 2023</td> <th>  Prob (F-statistic):</th> <td>1.61e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:10:24</td>     <th>  Log-Likelihood:    </th> <td> -7068.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>1.415e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   994</td>      <th>  BIC:               </th> <td>1.418e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                   <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                      <td>  450.7997</td> <td>   22.197</td> <td>   20.309</td> <td> 0.000</td> <td>  407.241</td> <td>  494.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Datos_D</th>                        <td>    2.2346</td> <td>   16.436</td> <td>    0.136</td> <td> 0.892</td> <td>  -30.019</td> <td>   34.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Col_a_codificar_grupos</th>         <td>   32.9796</td> <td>    6.480</td> <td>    5.089</td> <td> 0.000</td> <td>   20.263</td> <td>   45.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Datos_D:Col_a_codificar_grupos</th> <td>    4.0206</td> <td>    6.810</td> <td>    0.590</td> <td> 0.555</td> <td>   -9.342</td> <td>   17.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Datos_E</th>                        <td>   -0.0511</td> <td>    9.110</td> <td>   -0.006</td> <td> 0.996</td> <td>  -17.928</td> <td>   17.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Datos_F</th>                        <td>   -0.0495</td> <td>    0.031</td> <td>   -1.578</td> <td> 0.115</td> <td>   -0.111</td> <td>    0.012</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>297.679</td> <th>  Durbin-Watson:     </th> <td>   1.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  48.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.084</td>  <th>  Prob(JB):          </th> <td>3.62e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.939</td>  <th>  Cond. No.          </th> <td>1.46e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.46e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                Datos_G   R-squared:                       0.029\n",
       "Model:                            OLS   Adj. R-squared:                  0.025\n",
       "Method:                 Least Squares   F-statistic:                     6.042\n",
       "Date:                Sat, 11 Feb 2023   Prob (F-statistic):           1.61e-05\n",
       "Time:                        18:10:24   Log-Likelihood:                -7068.2\n",
       "No. Observations:                1000   AIC:                         1.415e+04\n",
       "Df Residuals:                     994   BIC:                         1.418e+04\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==================================================================================================\n",
       "                                     coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------------------\n",
       "Intercept                        450.7997     22.197     20.309      0.000     407.241     494.359\n",
       "Datos_D                            2.2346     16.436      0.136      0.892     -30.019      34.488\n",
       "Col_a_codificar_grupos            32.9796      6.480      5.089      0.000      20.263      45.696\n",
       "Datos_D:Col_a_codificar_grupos     4.0206      6.810      0.590      0.555      -9.342      17.384\n",
       "Datos_E                           -0.0511      9.110     -0.006      0.996     -17.928      17.826\n",
       "Datos_F                           -0.0495      0.031     -1.578      0.115      -0.111       0.012\n",
       "==============================================================================\n",
       "Omnibus:                      297.679   Durbin-Watson:                   1.962\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               48.086\n",
       "Skew:                           0.084   Prob(JB):                     3.62e-11\n",
       "Kurtosis:                       1.939   Cond. No.                     1.46e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.46e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interacciones entre variables que entre ellas hacen efecto en el outcome\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model=smf.ols(formula= \"Datos_G ~Datos_D*Col_a_codificar_grupos+Datos_E+Datos_F\",data=df_prueba )\n",
    "results=model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultado 66\n",
      "Datos_D   -0.021445\n",
      "Datos_E   -0.375779\n",
      "Datos_F         904\n",
      "Name: 157, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "\n",
    "# Outliers no es lo mismo que en la distribución porque aqui se usa el \n",
    "\n",
    "# cojo una categoria \n",
    "datos_cateC_1=df_prueba.loc[df_prueba[\"Datos_cate_C\"]==1,]\n",
    "datos_cateC_1=df_prueba.loc[df_prueba[\"Datos_cate_C\"]==1,]\n",
    "ej_outliers=sm.OLS(datos_cateC_1[OUTCOME], datos_cateC_1[predictores].assign(conts=1))\n",
    "resul_1=ej_outliers.fit()\n",
    "\n",
    "influence=OLSInfluence(resul_1)\n",
    "sresiduals= influence.resid_studentized_internal\n",
    "\n",
    "outliers=datos_cateC_1.loc[sresiduals.idxmin(), :]\n",
    "print(\"resultado\", outliers[OUTCOME])\n",
    "print(outliers[predictores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "609973a2d1f31d45d1c4d9f5c0b4ecf9cb33fe1a555b03392724c0cdbb5c54ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
